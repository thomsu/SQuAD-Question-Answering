{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lighting as pl\n",
    "from sklearn.model.selection import train_test_split\n",
    "from termcolor import colored\n",
    "import textwrap\n",
    "\n",
    "from transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_and_answers(factoid_path):\n",
    "    with factoid_path.open() as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    sections = data['data']\n",
    "    \n",
    "    for section in sections:\n",
    "        questions = section['paragraphs']\n",
    "        \n",
    "        data_rows = []\n",
    "        \n",
    "        for question in questions:\n",
    "            context = question['context']\n",
    "            for qa in question['qas']:\n",
    "                question = qa['question']\n",
    "                answers = qa['answers']\n",
    "                \n",
    "                for ans in answers:\n",
    "                    answer_text = answer['text']\n",
    "                    answer_start = answer['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                    \n",
    "                    data_rows.append([\n",
    "                        'question': question,\n",
    "                        'context': context,\n",
    "                        'answer_text': answer_text,\n",
    "                        'answer_start': answer_start.\n",
    "                        'answer_end': answer_end\n",
    "                    ])\n",
    "                    \n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_questions_and_answers(Path('train-v2.0.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_answer(question):\n",
    "    answer_start, answer_end = question['answer_start'], question['answer_end']\n",
    "    context = question['context']\n",
    "    \n",
    "    return colored(context[:answer_start], 'white') + colored(context[answer_start:answer_end+1], 'green') +\\\n",
    "colored(context[answer_end+1:], 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-benchmark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlearn-env",
   "language": "python",
   "name": "dlearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
